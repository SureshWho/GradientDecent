{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In machine learning a cost funtion, is a function which measures how good a model is, by comparing ground truth with the predicted result, lower this value, better the model is. In many machine learning problem, the objective is to  (mostly) minimmize the cost function.\n",
    "\n",
    "Let's take an example of logistic regression model, where prediction is defined by :\n",
    "\n",
    "$\\hat y = \\sigma (W^T x + b), \\sigma(Z) = {1 \\over {1 + e^{-z}}}$\n",
    "\n",
    "and the cost functions is defined by\n",
    "\n",
    "$ J(W,b) = -{1 \\over m} \\sum ^{m}_{i=1} y^{i} log (\\hat y^{i}) + (1 - y^{i}) log (1 - \\hat y^{i})$ \n",
    "\n",
    "In logistic regression the objective is to minimize $J(W,b)$, means to find right values of W and b, which minimize $J(W,b)$. In below picture Z axis (height) shows the error measured by the cost function, for all possible values of W and b. Here the objective, is to find right W, and b values, which minimize $J(W,b)$, that is to find the lowest point in the Z axis (marked by the circle at the bottom of the bowl).\n",
    "\n",
    "<img src=\"./images/gd.png\">\n",
    "\n",
    "This is an optimization problem, which can be solved in many ways, *Gradient Descent* is one of the most common way to solve this. Gradient descent, works as follows, Consider the \"bowl\" as a hill, and you want to reach the bottom :\n",
    "\n",
    "1. Start from a random point.\n",
    "1. Look around.\n",
    "1. Take a step towards direction of steepest descent.\n",
    "1. Repeat 3 and 4 till you reach the botton.\n",
    "\n",
    "<img src=\"./images/gdg.gif\">\n",
    "\n",
    "\n",
    "For simplicity, let's consider that, we want to minimize $J$, with only one parameter $J(w)$. Then, the single dimensional plot will look like this :\n",
    "\n",
    "<img src=\"./images/gd_single_para_1.png\">\n",
    "\n",
    "and *Gradient Descent* works as follow :\n",
    "\n",
    "```\n",
    "Initialize W to random value (or to zeros in case of logistic regression)\n",
    "Reate untill it converges\n",
    "    {\n",
    "```\n",
    "$\\hspace{4em}w = w - \\alpha {d \\over dw} \\: J(w)$ \n",
    " \n",
    "```\n",
    "    }\n",
    "```\n",
    "\n",
    "$\\alpha$ is learning rate, or size of the step taken in each ieration.\n",
    "\n",
    "${d \\over dw} \\: J(w)$ calculates slope of $J(w)$ at the given $w$. Each iternation, $w$ is reduced by $J(w)$'s slope at that point, which evatually reaches it's global minimum, or the lowest error point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative Intuition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIPS\n",
    "- Since logistic regression cost function in covex (bowl) any initialization (for W,b) method works fine. But, in general for logistic regression W and b are initialized to zeros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notations\n",
    "- $dw$ - is derivative, ${d \\over dw} \\: J(w)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- [Inspired by Prof Andrew Ng's course](https://www.coursera.org/learn/neural-networks-deep-learning/home/welcome)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
